#!/usr/bin/env bash

# fix root owner, just in case
chown root <%= node['hops']['home'] %>/etc/hadoop

# if nodemanager is running, exit
#systemctl status nodemanager
#if [ $? -ne 0 ] ; then

  # make sure GPUs are available
  which nvidia-smi
  if [ $? -eq 0 ] ; then
 
    num_gpus=$(nvidia-smi -L | wc -l)
    # how many gpus are configured in yarn-site.xml
    num_yarn_site=$(grep -C 2 '>yarn.nodemanager.resource.gpus<' <%= node['hops']['conf_dir'] %>/yarn-site.xml | grep value | sed 's/<value>//' | sed 's/<\/value>//' | tr -d '[:space:]')
    # if nvidia-smi and yarn-site.xml disagree on the number of GPUs, take number from nvidia-smi
    if [ "$num_gpus" != "$num_yarn_site" ] ; then
      # the nodemanager will recreate the correct number of cgroups. No cgroups should be running when this is executed
      rmdir /sys/fs/cgroup/devices/hops-yarn
      perl -i -p0e "s/yarn\.nodemanager\.resource\.gpus<\/name>.*<value>${num_yarn_site}/yarn\.nodemanager\.resource\.gpus<\/name>\n    <value>${num_gpus}/s" <%= node['hops']['conf_dir'] %>/yarn-site.xml
      chown <%= node['hops']['yarn']['user'] %> <%= node['hops']['home'] %>/etc/hadoop/yarn-site.xml
      chown root <%= node['hops']['home'] %>/etc/hadoop      
      echo "GPU configuration updated to the number of GPUs found by nvidia-smi"
    else
      echo "GPU configuration looks good."
    fi
  else
      echo "Could not find nvidia-smi program in the PATH"
      exit 0
  fi
#else
#    echo "Nodemanager is running. Cannot check gpu configuration if the nodemanager is running"
#    exit 0
#fi
exit 0
