#!/usr/bin/env bash

command=nodemanager
h=$(hostname)
bin=$(dirname "${BASH_SOURCE-$0}")
bin=$(cd "$bin"; pwd)
LOCALHOST=<%= node['install']['localhost'] %>
shopt -s nocasematch

if [ "${LOCALHOST}" == "true" ] ; then
    systemctl status nodemanager
    if [ $? -ne 0 ] ; then
	num_gpus=$(nvidia-smi -L | wc -l)
	num_yarn_site=$(grep -C 2 '>yarn.nodemanager.resource.gpus<' yarn-site.xml | grep value | sed 's/<value>//' | sed 's/<\/value>//' | tr -d '[:space:]')

	if [ $num_gpus -ne $num_yarn_site ] ; then
	    # the nodemanager will recreate the correct number of cgroups. No cgroups should be running when this is executed
	    rmdir /sys/fs/cgroup/devices/hops-yarn
	    # Replace the num_gpus in yarn-site.xml with the number discovered by 'nvidia-smi -L'
            perl -i -p0e "s/yarn\.nodemanager\.resource\.gpus<\/name>.*<value>${num_yarn_site}/yarn\.nodemanager\.resource\.gpus<\/name>\n    <value>${num_gpus}/s" <%= node['hops']['conf_dir'] %>/yarn-site.xml 
	fi
    fi
fi    

DEFAULT_LIBEXEC_DIR="$bin"/../libexec
HADOOP_LIBEXEC_DIR=${HADOOP_LIBEXEC_DIR:-$DEFAULT_LIBEXEC_DIR}
. "$HADOOP_LIBEXEC_DIR"/hadoop-config.sh
. "${bin}"/set-env.sh

export PATH=$PATH:/usr/local/cuda/bin
# nccl2 needed by horovod
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/lib64:/usr/local/nccl2/lib:/usr/lib64

log=<%= node['hops']['logs_dir'] %>/yarn-<%= node['hops']['yarn']['user'] %>-$command-$h.log

"$bin"/yarn-daemon.sh --config "$YARN_CONF_DIR"  start $command
sleep 2; head "$log"

PID_FILE=$YARN_PID_DIR/yarn-<%= node['hops']['yarn']['user'] %>-$command.pid
PID=$(cat "$PID_FILE")
kill -0 "$PID"

exit $?
